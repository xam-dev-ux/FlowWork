# Usar Ollama (IA Local Gratuita) en FlowWork

Ollama es una alternativa **100% gratuita** a OpenAI que corre completamente en tu mÃ¡quina local.

## Â¿Por quÃ© Ollama?

âœ… **Gratis** - sin API keys, sin costos
âœ… **Privado** - tus datos nunca salen de tu mÃ¡quina
âœ… **RÃ¡pido** - respuestas en <1 segundo
âœ… **Offline** - funciona sin internet
âœ… **Modelos potentes** - Llama 3.2, Mistral, Gemma, etc.

## InstalaciÃ³n

### Linux / WSL

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### macOS

```bash
brew install ollama
```

### Windows

Descarga desde: https://ollama.com/download

## ConfiguraciÃ³n para FlowWork

### 1. Iniciar Ollama

```bash
ollama serve
```

DeberÃ­a mostrar:
```
Listening on 127.0.0.1:11434
```

### 2. Descargar modelo (solo una vez)

```bash
# Llama 3.2 (recomendado, 2GB)
ollama pull llama3.2

# Alternativas:
# ollama pull mistral        # Mistral 7B
# ollama pull gemma2:2b      # Google Gemma (mÃ¡s pequeÃ±o)
# ollama pull qwen2.5:3b     # Qwen (mÃ¡s rÃ¡pido)
```

### 3. Probar el modelo

```bash
ollama run llama3.2
```

Escribe algo y deberÃ­a responder. Usa `/bye` para salir.

### 4. Actualizar el agente

Edita `agent/src/index.ts`:

```typescript
// ANTES (OpenAI - de pago)
import { parseIntent } from "./intentParser";

// DESPUÃ‰S (Ollama - gratis)
import { parseIntent } from "./intentParserLocal";
```

### 5. Actualizar .env del agente

```env
# ANTES
OPENAI_API_KEY=sk-...  # â† ELIMINAR

# DESPUÃ‰S
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

### 6. Reiniciar el agente

```bash
cd agent
npm start
```

## Uso

Ahora el agente procesarÃ¡ mensajes usando Ollama:

```
Usuario: "escribe copy para landing page, $25"
Ollama: â†’ Parsea intent localmente
Agent: â†’ Crea tarea en el contrato
```

## ComparaciÃ³n de Modelos

| Modelo | TamaÃ±o | RAM | Velocidad | Calidad |
|--------|--------|-----|-----------|---------|
| llama3.2 | 2GB | 4GB | Media | Alta |
| mistral | 4GB | 8GB | Media | Muy alta |
| gemma2:2b | 1.5GB | 3GB | RÃ¡pida | Media |
| qwen2.5:3b | 2GB | 4GB | Muy rÃ¡pida | Alta |

**RecomendaciÃ³n**: Empieza con `llama3.2`. Si tu mÃ¡quina tiene 16GB+ RAM, usa `mistral`.

## Cambiar de Modelo

```bash
# Descargar nuevo modelo
ollama pull mistral

# Actualizar .env
OLLAMA_MODEL=mistral

# Reiniciar agente
```

## Fallback sin IA

El cÃ³digo incluye un **fallback inteligente** por si Ollama no estÃ¡ corriendo:

```typescript
// Si Ollama falla, usa reglas simples:
"escribe copy, $20" â†’ create_task
"aprobar tarea 5" â†’ approve_delivery
```

Funciona sin IA, pero con menos precisiÃ³n.

## OptimizaciÃ³n de Rendimiento

### Mantener modelo en memoria

```bash
# Precarga el modelo (mÃ¡s rÃ¡pido)
ollama run llama3.2 &
```

### Ajustar configuraciÃ³n

Edita `intentParserLocal.ts`:

```typescript
{
  model: "llama3.2",
  stream: false,
  options: {
    temperature: 0,      // MÃ¡s determinÃ­stico
    num_predict: 100,    // Respuestas mÃ¡s cortas
    top_k: 10,          // MÃ¡s preciso
    top_p: 0.9
  }
}
```

## SoluciÃ³n de Problemas

### "Ollama no estÃ¡ corriendo"

```bash
# Verificar si estÃ¡ corriendo
curl http://localhost:11434

# Si no responde, iniciar:
ollama serve
```

### "Modelo no encontrado"

```bash
# Listar modelos instalados
ollama list

# Instalar el modelo
ollama pull llama3.2
```

### "Muy lento"

Prueba un modelo mÃ¡s pequeÃ±o:

```bash
ollama pull gemma2:2b
```

O aumenta la RAM asignada:

```bash
OLLAMA_MAX_LOADED_MODELS=1 ollama serve
```

### "Respuestas incorrectas"

Mejora el prompt en `intentParserLocal.ts`:

```typescript
const systemPrompt = `Eres un experto en anÃ¡lisis de intenciones...
[mÃ¡s ejemplos]
[mÃ¡s contexto]`;
```

## Ejecutar como Servicio (ProducciÃ³n)

### Linux (systemd)

```bash
sudo nano /etc/systemd/system/ollama.service
```

```ini
[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
User=YOUR_USER
ExecStart=/usr/local/bin/ollama serve
Restart=always

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl enable ollama
sudo systemctl start ollama
```

### Docker

```dockerfile
FROM ollama/ollama:latest

# Descargar modelo en build
RUN ollama pull llama3.2

EXPOSE 11434
CMD ["serve"]
```

## Ventajas vs OpenAI

| Aspecto | Ollama | OpenAI |
|---------|--------|--------|
| **Costo** | $0 | ~$0.002/request |
| **Privacidad** | 100% local | EnvÃ­a datos a OpenAI |
| **Latencia** | <1s | 1-3s |
| **Internet** | No necesario | Requerido |
| **Rate limits** | Sin lÃ­mites | 3 req/min (free) |
| **ConfiguraciÃ³n** | 5 minutos | Instant (con API key) |

## Recursos

- **DocumentaciÃ³n**: https://ollama.com/docs
- **Modelos**: https://ollama.com/library
- **GitHub**: https://github.com/ollama/ollama

## Ejemplos de Mensajes de Prueba

Prueba estos mensajes con el agente:

```
"necesito copy para landing page, $20"
"analiza estos datos CSV, $30 para maÃ±ana"
"traduce 500 palabras a inglÃ©s, $15"
"investiga sobre web3, $40"
"escribe post de twitter sobre IA, $10"
```

---

**Â¡Ya no necesitas pagar por OpenAI!** ðŸŽ‰

Con Ollama, FlowWork es 100% gratis y privado.
