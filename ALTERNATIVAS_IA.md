# Alternativas de IA Gratuitas para FlowWork

ComparaciÃ³n de todas las opciones gratuitas de IA que puedes usar en el agente.

## ğŸ† Recomendadas (Mejores)

### 1. Ollama (Local) â­ MEJOR OPCIÃ“N

**Ventajas:**
- âœ… 100% gratis, sin lÃ­mites
- âœ… Privacidad total (local)
- âœ… RÃ¡pido (<1s)
- âœ… Funciona offline
- âœ… FÃ¡cil de instalar

**Desventajas:**
- âŒ Necesita 4GB+ RAM
- âŒ InstalaciÃ³n inicial de 2GB

**InstalaciÃ³n:**
```bash
# Instalar Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Descargar modelo
ollama pull llama3.2

# Iniciar
ollama serve
```

**Uso:** Ver `OLLAMA_SETUP.md`

---

### 2. LM Studio (Local) â­

**Ventajas:**
- âœ… 100% gratis
- âœ… Interfaz grÃ¡fica (GUI)
- âœ… Descarga modelos con 1 clic
- âœ… Compatible con OpenAI API

**Desventajas:**
- âŒ Necesita 8GB+ RAM
- âŒ Solo Windows/Mac (no Linux)

**InstalaciÃ³n:**
1. Descarga: https://lmstudio.ai/
2. Abre LM Studio
3. Busca "Llama 3.2" â†’ Download
4. Start Server (puerto 1234)

**CÃ³digo:**
```typescript
// Usar LM Studio (compatible con OpenAI SDK)
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "http://localhost:1234/v1",
  apiKey: "not-needed"
});
```

---

### 3. Hugging Face Inference API (Cloud Gratis)

**Ventajas:**
- âœ… Gratis (hasta 1000 req/dÃ­a)
- âœ… Sin instalaciÃ³n
- âœ… No usa tu RAM
- âœ… Muchos modelos

**Desventajas:**
- âŒ Necesita internet
- âŒ MÃ¡s lento (2-5s)
- âŒ Rate limits

**InstalaciÃ³n:**
```bash
npm install @huggingface/inference
```

**CÃ³digo:**
```typescript
import { HfInference } from "@huggingface/inference";

const hf = new HfInference("hf_YOUR_FREE_TOKEN");

const result = await hf.textGeneration({
  model: "mistralai/Mistral-7B-Instruct-v0.2",
  inputs: prompt,
  parameters: {
    max_new_tokens: 200,
    temperature: 0.1,
  },
});
```

**Token gratis:** https://huggingface.co/settings/tokens

---

## Otras Opciones

### 4. Together.ai (Cloud)

**Gratis:** $25 de crÃ©dito inicial
**Modelos:** Llama 3, Mixtral, etc.
**CÃ³digo:**
```typescript
const response = await fetch("https://api.together.xyz/v1/chat/completions", {
  method: "POST",
  headers: {
    "Authorization": `Bearer ${TOGETHER_API_KEY}`,
    "Content-Type": "application/json"
  },
  body: JSON.stringify({
    model: "meta-llama/Llama-3-8b-chat-hf",
    messages: [{ role: "user", content: prompt }]
  })
});
```

### 5. Groq (Cloud - Muy RÃ¡pido)

**Gratis:** 14400 req/dÃ­a
**Velocidad:** <500ms (el mÃ¡s rÃ¡pido)
**Modelos:** Llama 3, Mixtral, Gemma

```bash
npm install groq-sdk
```

```typescript
import Groq from "groq-sdk";

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

const completion = await groq.chat.completions.create({
  model: "llama-3.1-8b-instant",
  messages: [{ role: "user", content: prompt }],
  temperature: 0,
});
```

**Registro:** https://console.groq.com/

### 6. Fireworks.ai

**Gratis:** $1 crÃ©dito inicial
**Ventaja:** API compatible con OpenAI

```typescript
import OpenAI from "openai";

const client = new OpenAI({
  baseURL: "https://api.fireworks.ai/inference/v1",
  apiKey: process.env.FIREWORKS_API_KEY,
});
```

---

## ComparaciÃ³n Completa

| OpciÃ³n | Costo | Velocidad | RAM Necesaria | Internet | Setup |
|--------|-------|-----------|---------------|----------|-------|
| **Ollama** | $0 | âš¡âš¡âš¡ | 4GB+ | No | 5 min |
| **LM Studio** | $0 | âš¡âš¡âš¡ | 8GB+ | No | 10 min |
| **HuggingFace** | $0 | âš¡âš¡ | 0 | SÃ­ | 1 min |
| **Groq** | $0 | âš¡âš¡âš¡âš¡ | 0 | SÃ­ | 2 min |
| **Together.ai** | $25 gratis | âš¡âš¡âš¡ | 0 | SÃ­ | 2 min |
| **OpenAI** | ~$0.002/req | âš¡âš¡âš¡ | 0 | SÃ­ | 1 min |

---

## RecomendaciÃ³n por Caso de Uso

### Para Desarrollo Local
â†’ **Ollama** (100% gratis, privado, rÃ¡pido)

### Para Prototipar RÃ¡pido
â†’ **Groq** (sÃºper rÃ¡pido, 14k req/dÃ­a gratis)

### Para ProducciÃ³n con Presupuesto
â†’ **Together.ai** o **Fireworks.ai** (econÃ³micos)

### Si tienes RAM limitada (<4GB)
â†’ **HuggingFace** o **Groq** (cloud)

---

## ImplementaciÃ³n en FlowWork

### OpciÃ³n 1: Solo Ollama (Recomendado)

```typescript
// agent/src/index.ts
import { parseIntent } from "./intentParserLocal"; // â† Ollama
```

### OpciÃ³n 2: Fallback MÃºltiple

```typescript
async function parseIntentWithFallback(message: string) {
  try {
    // Intentar Ollama primero (gratis, local)
    return await parseIntentOllama(message);
  } catch {
    try {
      // Fallback a Groq (gratis, cloud, rÃ¡pido)
      return await parseIntentGroq(message);
    } catch {
      // Fallback a reglas simples
      return parseIntentFallback(message);
    }
  }
}
```

### OpciÃ³n 3: Switch por Variable

```typescript
// agent/src/index.ts
const AI_PROVIDER = process.env.AI_PROVIDER || "ollama";

let parseIntent;
switch (AI_PROVIDER) {
  case "ollama":
    parseIntent = require("./intentParserLocal").parseIntent;
    break;
  case "groq":
    parseIntent = require("./intentParserGroq").parseIntent;
    break;
  case "openai":
    parseIntent = require("./intentParser").parseIntent;
    break;
  default:
    parseIntent = require("./intentParserLocal").parseIntent;
}
```

---

## Costos Estimados (1000 tareas/mes)

| Proveedor | Costo Mensual |
|-----------|---------------|
| Ollama | **$0** |
| LM Studio | **$0** |
| Groq | **$0** (dentro de lÃ­mites) |
| HuggingFace | **$0** (dentro de lÃ­mites) |
| Together.ai | ~$2 |
| OpenAI | ~$10 |

---

## InstalaciÃ³n RÃ¡pida de Ollama

```bash
# 1. Instalar
curl -fsSL https://ollama.com/install.sh | sh

# 2. Descargar modelo
ollama pull llama3.2

# 3. Probar
ollama run llama3.2
>>> escribe un haiku sobre blockchain
>>> /bye

# 4. Iniciar como servicio
ollama serve &

# 5. Usar en FlowWork
# Editar agent/src/index.ts para usar intentParserLocal.ts
```

**Â¡Listo!** Ya tienes IA gratis funcionando. ğŸ‰

---

## Recursos

- **Ollama**: https://ollama.com
- **LM Studio**: https://lmstudio.ai
- **Groq**: https://console.groq.com
- **HuggingFace**: https://huggingface.co/inference-api
- **Together.ai**: https://together.ai

---

**Mi recomendaciÃ³n personal: Empieza con Ollama.**

Es el equilibrio perfecto entre gratis, privado, rÃ¡pido y fÃ¡cil de usar.
